{"cells": [{"cell_type": "markdown", "metadata": {}, "source": [" Fake News Detection with Explainability (BERT + SHAP + LIME)\n", "\n", "This notebook builds a high-performance, interpretable Fake News classifier using BERT with SHAP and LIME for explainability."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udce6 Install required libraries\n", "!pip install transformers shap lime datasets --quiet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [" Imports\n", "import numpy as np\n", "import pandas as pd\n", "import shap\n", "import torch\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import classification_report, confusion_matrix\n", "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n", "from transformers import DataCollatorWithPadding\n", "from datasets import Dataset\n", "from lime.lime_text import LimeTextExplainer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 Device check\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(\"Using:\", device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udd39 Load LIAR dataset sample\n", "df = pd.read_csv('https://raw.githubusercontent.com/saurabhmathur96/LIAR-Dataset/main/train.tsv', sep='\\t', header=None)\n", "df = df[[2, 1]]\n", "df.columns = ['text', 'label']\n", "df['label'] = df['label'].replace({\n", "    'false': 0, 'pants-fire': 0, 'barely-true': 0,\n", "    'half-true': 1, 'mostly-true': 1, 'true': 1\n", "})\n", "df = df.sample(5000, random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udcca Train-Test Split\n", "train_texts, val_texts, train_labels, val_labels = train_test_split(\n", "    df['text'].tolist(), df['label'].tolist(), test_size=0.15, stratify=df['label'], random_state=42)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83e\udde0 Tokenizer\n", "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n", "\n", "def tokenize_data(texts):\n", "    return tokenizer(texts, truncation=True, padding=True, max_length=128)\n", "\n", "train_encodings = tokenize_data(train_texts)\n", "val_encodings = tokenize_data(val_texts)\n", "\n", "train_dataset = Dataset.from_dict({**train_encodings, \"label\": train_labels})\n", "val_dataset = Dataset.from_dict({**val_encodings, \"label\": val_labels})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83e\udd16 Load Model\n", "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udee0\ufe0f Training Configuration\n", "training_args = TrainingArguments(\n", "    output_dir='./results',\n", "    evaluation_strategy=\"epoch\",\n", "    save_strategy=\"epoch\",\n", "    learning_rate=2e-5,\n", "    per_device_train_batch_size=16,\n", "    per_device_eval_batch_size=16,\n", "    num_train_epochs=4,\n", "    weight_decay=0.01,\n", "    load_best_model_at_end=True,\n", "    save_total_limit=1\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udce6 Trainer Setup\n", "trainer = Trainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=train_dataset,\n", "    eval_dataset=val_dataset,\n", "    tokenizer=tokenizer,\n", "    data_collator=DataCollatorWithPadding(tokenizer)\n", ")\n", "\n", "# \ud83d\ude80 Train\n", "trainer.train()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udcc8 Evaluate\n", "preds = trainer.predict(val_dataset)\n", "y_pred = np.argmax(preds.predictions, axis=1)\n", "print(\"Classification Report:\\n\", classification_report(val_labels, y_pred))\n", "print(\"Confusion Matrix:\\n\", confusion_matrix(val_labels, y_pred))"]}], "metadata": {"colab": {"name": "Fake_News_Detection_with_Explainability.ipynb"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 0}
